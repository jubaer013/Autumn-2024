{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"V0HCQyV5vZLF"},"outputs":[],"source":["from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.naive_bayes import GaussianNB\n","import xgboost as xgb\n","from imblearn.over_sampling import SMOTE, ADASYN\n","import numpy as np\n","from sklearn.model_selection import cross_val_score\n"]},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"0_ZdekyL9Xui"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv(\"water_potability.csv\")"],"metadata":{"id":"tIoQGnkt9cqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwEoynjv0iCP","outputId":"2752e8b6-e824-4c27-c953-92bf1471604b"},"outputs":[{"data":{"text/plain":["0.0    1998\n","1.0    1278\n","Name: Potability, dtype: int64"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["\n","\n","# Imputing into null values\n","imputer = SimpleImputer(strategy='mean')\n","data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n","data[\"Potability\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMEfkjJ3TpU_"},"outputs":[],"source":["zero  = data[data['Potability']==0]   #zero values in Potability column\n","one = data[data['Potability']==1]  # one values in Potability column\n","\n","from sklearn.utils import resample\n","# minority class that  is 1, we need to upsample/increase that class so that there is no bias\n","# n_samples = 1998 means we want 1998 sample of class 1, since there are 1998 samples of class 0\n","data_minority_upsampled = resample(one, replace = True, n_samples = 1998)\n","#concatenate\n","data = pd.concat([zero, data_minority_upsampled])\n","\n","# X_raw = data[['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', , 'Trihalomethanes', 'Turbidity']]\n","X_raw = data[['ph', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Turbidity']]\n","y_raw = data['Potability']\n","\n","#from imblearn.over_sampling import BorderlineSMOTE\n","#X, y = ADASYN().fit_resample(X_raw, y_raw)\n","\n","#from sklearn.utils import shuffle\n","#data = shuffle(data) # shuffling so that there is particular sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pyV_qObRUlQZ","outputId":"9d2f1392-0816-40dd-9531-0841ed517429"},"outputs":[{"data":{"text/plain":["ph                 0\n","Hardness           0\n","Solids             0\n","Chloramines        0\n","Sulfate            0\n","Conductivity       0\n","Organic_carbon     0\n","Trihalomethanes    0\n","Turbidity          0\n","Potability         0\n","dtype: int64"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["data.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9RTh7pTr1uZy"},"outputs":[],"source":["# Preprocessing 01\n","\n","# X = data_imputed[['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']]\n","\n","X = data[['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']]\n","y = data['Potability']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","scaler = StandardScaler()\n","# scaler = MinMaxScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9F5vrpJU5i4J"},"outputs":[],"source":["# Preprocessing 02\n","\n","def imputing(df):\n","  for col in df.columns:\n","      if df[col].isnull().any():\n","          median_col = df[col].median()\n","          df[col].fillna(median_col, inplace=True)\n","  return df\n","\n","df = imputing(data)\n","\n","# Define X and y\n","X = df.drop('Potability', axis=1)\n","y = df['Potability']\n","\n","# Apply log transformation only to features\n","X_log_transformed = np.log1p(X)\n","\n","\n","# Apply SMOTE only to the target variable\n","smote = SMOTE(random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X_log_transformed, y)\n","\n","# Scale the features\n","scaler = RobustScaler()\n","X_scaled = scaler.fit_transform(X_resampled)\n","\n","# Split the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFKcb7h49TQS"},"outputs":[],"source":["Xs = scaler.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHHQHlv59TQS","outputId":"919c48d9-2674-497e-9a01-0a9b763b2679"},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected Feature 1: 1, Mean Accuracy: 0.7322\n","Selected Feature 2: 2, Mean Accuracy: 0.7487\n","Selected Feature 3: 0, Mean Accuracy: 0.7690\n","Selected Feature 4: 4, Mean Accuracy: 0.7848\n","Selected Feature 5: 3, Mean Accuracy: 0.7975\n"]}],"source":["# Feature Selection - Forward Selection\n","\n","selected_features = []\n","rf_model = RandomForestClassifier()\n","\n","num_features_to_select = 5\n","\n","while len(selected_features) < num_features_to_select:\n","    best_score = -1\n","    best_feature = None\n","\n","    for feature_idx in range(Xs.shape[1]):\n","        if feature_idx in selected_features:\n","            continue\n","\n","        # Try adding the feature to the selected set\n","        candidate_features = selected_features + [feature_idx]\n","\n","        # Evaluate the model's performance using cross-validation\n","        scores = cross_val_score(rf_model, Xs[:, candidate_features], y, cv=5, scoring='accuracy')\n","        mean_score = np.mean(scores)\n","\n","        # Keep track of the best-performing feature\n","        if mean_score > best_score:\n","            best_score = mean_score\n","            best_feature = feature_idx\n","\n","    if best_feature is not None:\n","        selected_features.append(best_feature)\n","        print(f\"Selected Feature {len(selected_features)}: {best_feature}, Mean Accuracy: {best_score:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oP-k-6s79TQS"},"outputs":[],"source":["X_train = X_train[:, 0:9]\n","X_test = X_test[:, 0:9]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXxiVhSa6Npz","outputId":"f961837b-d801-46c4-b0ea-057d2c33f71b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.82\n","[ 11  13  14  25  30  36  38  39  40  43  44  50  61  87  88  93  94 104\n"," 117 124 129 134 137 138 152 154 161 181 183 188 191 199 202 207 208 220\n"," 225 228 232 250 261 265 277 287 294 299 300 303 307 308 310 311 316 325\n"," 326 330 332 337 340 341 361 365 367 371 376 380 384 387 394 397 406 408\n"," 415 417 432 434 448 451 479 488 489 494 500 506 514 520 523 528 529 532\n"," 533 536 543 545 548 549 562 564 572 576 582 583 586 591 594 600 606 608\n"," 612 615 620 625 627 634 646 655 657 660 664 668 676 678 691 701 704 707\n"," 712 716 722 725 729 738 741 746 748 757 758 764 774 790 799]\n"]}],"source":["#Random forest\n","rf_model = RandomForestClassifier()\n","rf_model.fit(X_train, y_train)\n","y_pred = rf_model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n","mismatch_indices_rf = np.where(y_pred != y_test)[0]\n","print(mismatch_indices_rf)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2krHjTcDUbbC","outputId":"d5736aa5-244d-4e20-e332-3b6fa5a3a097"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.64\n","[  0   4   6   8  10  11  12  18  20  21  24  29  30  33  36  38  41  43\n","  46  47  50  52  54  55  60  65  66  69  71  72  74  79  81  82  84  88\n","  91  94  95  96  98 104 105 107 109 110 115 119 121 124 125 128 131 133\n"," 137 140 142 146 148 149 150 158 164 165 168 172 174 184 186 188 189 192\n"," 193 195 202 205 206 209 211 212 214 219 220 227 228 230 236 238 240 248\n"," 249 253 259 263 267 268 273 281 282 283 286 287 294 297 310 318 321 322\n"," 327 329 333 334 338 341 342 343 347 348 351 359 360 366 369 370 375 377\n"," 380 383 384 388 389 392 393 394 395 397 399 401 402 404 405 406 412 413\n"," 416 417 421 424 425 426 428 430 432 433 434 435 438 439 444 445 453 455\n"," 457 458 460 466 469 470 471 476 478 480 482 488 492 497 498 499 502 505\n"," 509 511]\n"]}],"source":["#svm\n","svm_model = SVC()\n","svm_model.fit(X_train, y_train)\n","y_pred = svm_model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n","mismatch_indices_svm = np.where(y_pred != y_test)[0]\n","print(mismatch_indices_svm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5s0mzJDXyMNx","outputId":"a4714d9e-69c9-42f1-d90e-a0cf7bd4d84f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n","100/100 [==============================] - 2s 6ms/step - loss: 0.6876 - accuracy: 0.5432 - val_loss: 0.6641 - val_accuracy: 0.6062\n","Epoch 2/200\n","100/100 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.5951 - val_loss: 0.6451 - val_accuracy: 0.6325\n","Epoch 3/200\n","100/100 [==============================] - 0s 5ms/step - loss: 0.6518 - accuracy: 0.6180 - val_loss: 0.6341 - val_accuracy: 0.6413\n","Epoch 4/200\n","100/100 [==============================] - 0s 4ms/step - loss: 0.6425 - accuracy: 0.6151 - val_loss: 0.6263 - val_accuracy: 0.6525\n","Epoch 5/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.6294 - accuracy: 0.6298 - val_loss: 0.6201 - val_accuracy: 0.6575\n","Epoch 6/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.6175 - accuracy: 0.6436 - val_loss: 0.6156 - val_accuracy: 0.6562\n","Epoch 7/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.6177 - accuracy: 0.6380 - val_loss: 0.6139 - val_accuracy: 0.6475\n","Epoch 8/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.6162 - accuracy: 0.6483 - val_loss: 0.6113 - val_accuracy: 0.6575\n","Epoch 9/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.6066 - accuracy: 0.6605 - val_loss: 0.6103 - val_accuracy: 0.6575\n","Epoch 10/200\n","100/100 [==============================] - 0s 4ms/step - loss: 0.6166 - accuracy: 0.6508 - val_loss: 0.6088 - val_accuracy: 0.6575\n","Epoch 11/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5997 - accuracy: 0.6712 - val_loss: 0.6051 - val_accuracy: 0.6675\n","Epoch 12/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.6045 - accuracy: 0.6661 - val_loss: 0.6050 - val_accuracy: 0.6637\n","Epoch 13/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.6078 - accuracy: 0.6564 - val_loss: 0.6050 - val_accuracy: 0.6525\n","Epoch 14/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.6031 - accuracy: 0.6608 - val_loss: 0.6034 - val_accuracy: 0.6687\n","Epoch 15/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5979 - accuracy: 0.6661 - val_loss: 0.6044 - val_accuracy: 0.6687\n","Epoch 16/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5929 - accuracy: 0.6805 - val_loss: 0.5998 - val_accuracy: 0.6650\n","Epoch 17/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5941 - accuracy: 0.6783 - val_loss: 0.5995 - val_accuracy: 0.6725\n","Epoch 18/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5927 - accuracy: 0.6799 - val_loss: 0.5973 - val_accuracy: 0.6575\n","Epoch 19/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5906 - accuracy: 0.6799 - val_loss: 0.5938 - val_accuracy: 0.6662\n","Epoch 20/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5890 - accuracy: 0.6787 - val_loss: 0.5948 - val_accuracy: 0.6600\n","Epoch 21/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5845 - accuracy: 0.6787 - val_loss: 0.5924 - val_accuracy: 0.6637\n","Epoch 22/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5863 - accuracy: 0.6862 - val_loss: 0.5984 - val_accuracy: 0.6775\n","Epoch 23/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5786 - accuracy: 0.6909 - val_loss: 0.5912 - val_accuracy: 0.6625\n","Epoch 24/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5779 - accuracy: 0.6906 - val_loss: 0.5921 - val_accuracy: 0.6812\n","Epoch 25/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5848 - accuracy: 0.6740 - val_loss: 0.5900 - val_accuracy: 0.6787\n","Epoch 26/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5725 - accuracy: 0.7021 - val_loss: 0.5897 - val_accuracy: 0.6775\n","Epoch 27/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5776 - accuracy: 0.6896 - val_loss: 0.5882 - val_accuracy: 0.6837\n","Epoch 28/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5693 - accuracy: 0.7012 - val_loss: 0.5886 - val_accuracy: 0.6762\n","Epoch 29/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5698 - accuracy: 0.6974 - val_loss: 0.5873 - val_accuracy: 0.6725\n","Epoch 30/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5712 - accuracy: 0.6921 - val_loss: 0.5873 - val_accuracy: 0.6787\n","Epoch 31/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5704 - accuracy: 0.6937 - val_loss: 0.5870 - val_accuracy: 0.6812\n","Epoch 32/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5715 - accuracy: 0.6965 - val_loss: 0.5843 - val_accuracy: 0.6862\n","Epoch 33/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5702 - accuracy: 0.6962 - val_loss: 0.5864 - val_accuracy: 0.6762\n","Epoch 34/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5688 - accuracy: 0.6918 - val_loss: 0.5836 - val_accuracy: 0.6850\n","Epoch 35/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5666 - accuracy: 0.6940 - val_loss: 0.5844 - val_accuracy: 0.6837\n","Epoch 36/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5543 - accuracy: 0.7106 - val_loss: 0.5814 - val_accuracy: 0.6862\n","Epoch 37/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5529 - accuracy: 0.7049 - val_loss: 0.5809 - val_accuracy: 0.6925\n","Epoch 38/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5536 - accuracy: 0.7074 - val_loss: 0.5793 - val_accuracy: 0.6925\n","Epoch 39/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5637 - accuracy: 0.7081 - val_loss: 0.5803 - val_accuracy: 0.6837\n","Epoch 40/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5530 - accuracy: 0.7059 - val_loss: 0.5826 - val_accuracy: 0.6888\n","Epoch 41/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5601 - accuracy: 0.7096 - val_loss: 0.5804 - val_accuracy: 0.6825\n","Epoch 42/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5605 - accuracy: 0.7031 - val_loss: 0.5801 - val_accuracy: 0.6850\n","Epoch 43/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5547 - accuracy: 0.7099 - val_loss: 0.5791 - val_accuracy: 0.6800\n","Epoch 44/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5553 - accuracy: 0.7059 - val_loss: 0.5788 - val_accuracy: 0.6850\n","Epoch 45/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5539 - accuracy: 0.7009 - val_loss: 0.5801 - val_accuracy: 0.6812\n","Epoch 46/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5532 - accuracy: 0.7081 - val_loss: 0.5785 - val_accuracy: 0.6850\n","Epoch 47/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5426 - accuracy: 0.7115 - val_loss: 0.5739 - val_accuracy: 0.6862\n","Epoch 48/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5510 - accuracy: 0.7096 - val_loss: 0.5724 - val_accuracy: 0.6988\n","Epoch 49/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5438 - accuracy: 0.7168 - val_loss: 0.5754 - val_accuracy: 0.6862\n","Epoch 50/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5464 - accuracy: 0.7140 - val_loss: 0.5736 - val_accuracy: 0.6963\n","Epoch 51/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5463 - accuracy: 0.7153 - val_loss: 0.5727 - val_accuracy: 0.7000\n","Epoch 52/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5486 - accuracy: 0.7159 - val_loss: 0.5730 - val_accuracy: 0.6925\n","Epoch 53/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5364 - accuracy: 0.7193 - val_loss: 0.5729 - val_accuracy: 0.7025\n","Epoch 54/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5441 - accuracy: 0.7196 - val_loss: 0.5702 - val_accuracy: 0.6975\n","Epoch 55/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5395 - accuracy: 0.7212 - val_loss: 0.5698 - val_accuracy: 0.7050\n","Epoch 56/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7384 - val_loss: 0.5721 - val_accuracy: 0.7013\n","Epoch 57/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.7218 - val_loss: 0.5684 - val_accuracy: 0.7063\n","Epoch 58/200\n","100/100 [==============================] - 0s 4ms/step - loss: 0.5412 - accuracy: 0.7165 - val_loss: 0.5725 - val_accuracy: 0.7000\n","Epoch 59/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7225 - val_loss: 0.5707 - val_accuracy: 0.7100\n","Epoch 60/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5273 - accuracy: 0.7334 - val_loss: 0.5677 - val_accuracy: 0.7063\n","Epoch 61/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5243 - accuracy: 0.7378 - val_loss: 0.5664 - val_accuracy: 0.7063\n","Epoch 62/200\n","100/100 [==============================] - 0s 4ms/step - loss: 0.5272 - accuracy: 0.7331 - val_loss: 0.5661 - val_accuracy: 0.7063\n","Epoch 63/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7278 - val_loss: 0.5655 - val_accuracy: 0.6963\n","Epoch 64/200\n","100/100 [==============================] - 0s 4ms/step - loss: 0.5348 - accuracy: 0.7218 - val_loss: 0.5671 - val_accuracy: 0.7100\n","Epoch 65/200\n","100/100 [==============================] - 0s 4ms/step - loss: 0.5224 - accuracy: 0.7369 - val_loss: 0.5680 - val_accuracy: 0.7113\n","Epoch 66/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5271 - accuracy: 0.7268 - val_loss: 0.5658 - val_accuracy: 0.7088\n","Epoch 67/200\n","100/100 [==============================] - 0s 4ms/step - loss: 0.5228 - accuracy: 0.7409 - val_loss: 0.5669 - val_accuracy: 0.7100\n","Epoch 68/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5246 - accuracy: 0.7300 - val_loss: 0.5661 - val_accuracy: 0.7150\n","Epoch 69/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5249 - accuracy: 0.7369 - val_loss: 0.5669 - val_accuracy: 0.7100\n","Epoch 70/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.7293 - val_loss: 0.5643 - val_accuracy: 0.7100\n","Epoch 71/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5246 - accuracy: 0.7353 - val_loss: 0.5628 - val_accuracy: 0.7150\n","Epoch 72/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5279 - accuracy: 0.7300 - val_loss: 0.5628 - val_accuracy: 0.7163\n","Epoch 73/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5203 - accuracy: 0.7325 - val_loss: 0.5637 - val_accuracy: 0.7175\n","Epoch 74/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5193 - accuracy: 0.7412 - val_loss: 0.5615 - val_accuracy: 0.7138\n","Epoch 75/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7422 - val_loss: 0.5646 - val_accuracy: 0.7100\n","Epoch 76/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5210 - accuracy: 0.7334 - val_loss: 0.5590 - val_accuracy: 0.7163\n","Epoch 77/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7456 - val_loss: 0.5609 - val_accuracy: 0.7075\n","Epoch 78/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7378 - val_loss: 0.5607 - val_accuracy: 0.7212\n","Epoch 79/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5213 - accuracy: 0.7353 - val_loss: 0.5611 - val_accuracy: 0.7125\n","Epoch 80/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5162 - accuracy: 0.7369 - val_loss: 0.5650 - val_accuracy: 0.7188\n","Epoch 81/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5160 - accuracy: 0.7312 - val_loss: 0.5644 - val_accuracy: 0.7125\n","Epoch 82/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7509 - val_loss: 0.5653 - val_accuracy: 0.7212\n","Epoch 83/200\n","100/100 [==============================] - 0s 3ms/step - loss: 0.5104 - accuracy: 0.7516 - val_loss: 0.5646 - val_accuracy: 0.7100\n","Epoch 84/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7409 - val_loss: 0.5606 - val_accuracy: 0.7237\n","Epoch 85/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7544 - val_loss: 0.5604 - val_accuracy: 0.7175\n","Epoch 86/200\n","100/100 [==============================] - 0s 2ms/step - loss: 0.5148 - accuracy: 0.7453 - val_loss: 0.5635 - val_accuracy: 0.7225\n"]}],"source":["# Neural Network 01\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Build the Sequential model\n","model_nn = Sequential([\n","    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n","    Dropout(0.2),  # Optional dropout layer for regularization\n","    Dense(32, activation='relu'),\n","    Dropout(0.2),\n","    Dense(1, activation='sigmoid')  # Output layer for binary classification\n","])\n","\n","# Compile the model\n","model_nn.compile(optimizer=Adam(learning_rate=0.001),  # Adjust learning rate as needed\n","              loss='binary_crossentropy',  # Binary crossentropy for binary classification\n","              metrics=['accuracy'])\n","\n","# Define early stopping to prevent overfitting\n","early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","# Train the model\n","history = model_nn.fit(X_train, y_train,\n","                    epochs=200,  # Adjust epochs as needed\n","                    batch_size=32,  # Adjust batch size as needed\n","                    validation_data=(X_test, y_test),\n","                    callbacks=[early_stop])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tVZ7WoryhPH","outputId":"700ab231-0205-4e37-8d00-9c983c9a7929"},"outputs":[{"name":"stdout","output_type":"stream","text":["31/31 [==============================] - 0s 1ms/step\n","Accuracy: 0.68\n","[  1   4   7   8  11  13  20  21  27  45  50  51  53  61  62  63  64  66\n","  67  68  71  77  80  81  84  87  90  92  98  99 100 102 107 108 115 116\n"," 127 130 133 134 137 139 142 144 146 147 166 173 177 180 181 184 186 188\n"," 189 191 193 194 199 200 201 206 212 217 219 221 224 227 232 235 237 242\n"," 244 246 247 250 251 252 253 254 255 256 259 261 264 267 272 279 282 285\n"," 287 290 291 292 293 294 296 298 303 305 307 308 310 315 316 317 322 325\n"," 327 329 334 337 342 343 348 354 356 359 369 371 373 378 379 381 382 383\n"," 384 389 398 407 408 411 412 413 417 418 419 421 422 424 427 430 433 434\n"," 438 439 440 444 449 458 460 461 463 466 467 470 471 472 477 479 480 481\n"," 489 490 491 492 494 496 501 504 510 511 516 517 520 522 523 524 526 531\n"," 535 543 552 553 554 559 562 564 566 572 573 575 576 579 582 583 586 588\n"," 593 595 598 600 602 609 610 612 615 624 631 636 638 640 659 662 663 664\n"," 673 679 681 682 683 685 686 688 691 692 693 695 700 704 705 706 711 722\n"," 723 725 730 731 732 736 739 741 745 751 754 755 759 763 765 766 773 778\n"," 780 781 783 784 785 787 789 795 796 803 804 809 816 818 822 825 829 830\n"," 835 839 840 843 847 853 854 858 862 865 866 869 874 876 883 884 887 888\n"," 890 894 895 896 915 917 926 927 932 933 934 937 939 941 945 953 954 959\n"," 965 974 976 979 980 981]\n"]}],"source":["# Evaluate the model\n","y_pred = model_nn.predict(X_test_scaled)\n","y_pred_bin = []\n","for i in y_pred:\n","  if i<0.5:\n","    y_pred_bin.append(0)\n","  else:\n","    y_pred_bin.append(1)\n","\n","accuracy = accuracy_score(y_test, y_pred_bin)\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n","mismatch_indices_nn = np.where(y_pred_bin != y_test)[0]\n","print(mismatch_indices_nn)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"u4jL_nhsmZxo","outputId":"e444e8e0-1108-4fb0-9a50-d085c26f6bdd"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 4, 522, 11, 523, 13, 524, 526, 531, 20, 535, 27, 543, 553, 554, 45, 559, 50, 51, 562, 53, 572, 61, 62, 573, 64, 66, 67, 68, 579, 582, 71, 583, 586, 77, 81, 84, 598, 87, 90, 602, 92, 609, 610, 99, 100, 102, 614, 615, 107, 108, 624, 115, 116, 628, 631, 636, 638, 127, 641, 130, 134, 137, 139, 652, 142, 144, 147, 663, 668, 166, 681, 682, 683, 173, 685, 686, 177, 691, 180, 181, 182, 692, 693, 695, 186, 188, 189, 704, 193, 194, 705, 706, 711, 200, 201, 717, 722, 213, 725, 217, 730, 731, 221, 736, 227, 739, 741, 745, 235, 751, 242, 754, 244, 755, 246, 247, 251, 763, 765, 254, 769, 259, 261, 264, 267, 780, 781, 783, 784, 785, 787, 789, 795, 285, 801, 291, 292, 293, 803, 804, 296, 807, 298, 809, 816, 818, 307, 308, 310, 825, 315, 316, 829, 830, 322, 835, 325, 327, 839, 329, 840, 843, 334, 337, 342, 343, 854, 858, 862, 865, 354, 866, 869, 359, 874, 876, 367, 369, 881, 371, 883, 373, 884, 888, 378, 379, 381, 382, 383, 894, 895, 896, 389, 398, 917, 407, 408, 411, 412, 413, 926, 927, 417, 419, 932, 421, 422, 934, 424, 937, 427, 941, 430, 433, 435, 439, 953, 954, 956, 959, 449, 965, 458, 460, 461, 974, 467, 979, 980, 470, 981, 473, 481, 489, 490, 492, 494, 496, 504, 510, 511]\n","Common Mismatched Predictions:\n"]},{"data":{"text/html":["\n","  <div id=\"df-d38cc159-92c3-4f74-a5f9-822473d9dbd8\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Actual</th>\n","      <th>ph</th>\n","      <th>Hardness</th>\n","      <th>Solids</th>\n","      <th>Chloramines</th>\n","      <th>Sulfate</th>\n","      <th>Conductivity</th>\n","      <th>Organic_carbon</th>\n","      <th>Trihalomethanes</th>\n","      <th>Turbidity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>6.643159</td>\n","      <td>188.913541</td>\n","      <td>32873.820022</td>\n","      <td>6.791509</td>\n","      <td>333.848842</td>\n","      <td>336.561501</td>\n","      <td>14.706810</td>\n","      <td>67.844849</td>\n","      <td>4.562198</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>6.615350</td>\n","      <td>179.240661</td>\n","      <td>26392.863612</td>\n","      <td>9.309160</td>\n","      <td>333.775777</td>\n","      <td>496.363562</td>\n","      <td>12.786595</td>\n","      <td>78.262369</td>\n","      <td>4.453443</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>5.628407</td>\n","      <td>226.830043</td>\n","      <td>28334.491937</td>\n","      <td>8.012497</td>\n","      <td>293.924273</td>\n","      <td>351.161213</td>\n","      <td>15.019241</td>\n","      <td>35.178662</td>\n","      <td>5.050683</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>7.714313</td>\n","      <td>191.692640</td>\n","      <td>19789.636080</td>\n","      <td>7.856557</td>\n","      <td>340.326314</td>\n","      <td>377.926368</td>\n","      <td>18.917027</td>\n","      <td>58.692204</td>\n","      <td>4.425134</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>7.830608</td>\n","      <td>213.249258</td>\n","      <td>18821.389612</td>\n","      <td>6.475113</td>\n","      <td>316.572415</td>\n","      <td>405.278738</td>\n","      <td>16.872395</td>\n","      <td>55.905770</td>\n","      <td>3.551568</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1.0</td>\n","      <td>6.350290</td>\n","      <td>190.383738</td>\n","      <td>14905.393852</td>\n","      <td>5.537830</td>\n","      <td>333.775777</td>\n","      <td>446.840605</td>\n","      <td>13.983567</td>\n","      <td>67.817096</td>\n","      <td>4.265233</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1.0</td>\n","      <td>6.597292</td>\n","      <td>191.787442</td>\n","      <td>25039.354696</td>\n","      <td>7.294577</td>\n","      <td>395.739529</td>\n","      <td>501.532653</td>\n","      <td>14.695391</td>\n","      <td>80.050031</td>\n","      <td>4.318305</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1.0</td>\n","      <td>6.903074</td>\n","      <td>206.922504</td>\n","      <td>17947.988114</td>\n","      <td>7.048017</td>\n","      <td>333.775777</td>\n","      <td>601.985223</td>\n","      <td>11.775110</td>\n","      <td>58.176255</td>\n","      <td>4.473887</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1.0</td>\n","      <td>7.736313</td>\n","      <td>225.063103</td>\n","      <td>19496.848592</td>\n","      <td>7.158343</td>\n","      <td>289.945985</td>\n","      <td>433.974022</td>\n","      <td>15.153817</td>\n","      <td>74.765101</td>\n","      <td>3.700917</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.0</td>\n","      <td>6.380717</td>\n","      <td>266.015410</td>\n","      <td>21250.935634</td>\n","      <td>4.854335</td>\n","      <td>357.241027</td>\n","      <td>358.185473</td>\n","      <td>27.006707</td>\n","      <td>59.937785</td>\n","      <td>4.532020</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1.0</td>\n","      <td>7.890354</td>\n","      <td>180.158098</td>\n","      <td>32160.533923</td>\n","      <td>7.773484</td>\n","      <td>360.283983</td>\n","      <td>344.550619</td>\n","      <td>11.246460</td>\n","      <td>60.292187</td>\n","      <td>3.209588</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1.0</td>\n","      <td>9.318614</td>\n","      <td>317.338124</td>\n","      <td>24497.873935</td>\n","      <td>7.597452</td>\n","      <td>357.167217</td>\n","      <td>476.510384</td>\n","      <td>12.032377</td>\n","      <td>68.599830</td>\n","      <td>4.642719</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1.0</td>\n","      <td>6.866357</td>\n","      <td>185.631487</td>\n","      <td>13979.381750</td>\n","      <td>7.043425</td>\n","      <td>375.434032</td>\n","      <td>462.535115</td>\n","      <td>17.194992</td>\n","      <td>74.557790</td>\n","      <td>4.346154</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1.0</td>\n","      <td>7.017295</td>\n","      <td>170.132446</td>\n","      <td>20169.344252</td>\n","      <td>5.216280</td>\n","      <td>378.226971</td>\n","      <td>512.768439</td>\n","      <td>15.505992</td>\n","      <td>64.624944</td>\n","      <td>4.899611</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1.0</td>\n","      <td>5.827541</td>\n","      <td>191.004930</td>\n","      <td>25863.343245</td>\n","      <td>4.823832</td>\n","      <td>316.025513</td>\n","      <td>370.357288</td>\n","      <td>18.361858</td>\n","      <td>48.658109</td>\n","      <td>3.179129</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1.0</td>\n","      <td>7.080795</td>\n","      <td>173.474186</td>\n","      <td>18318.720372</td>\n","      <td>7.597654</td>\n","      <td>282.598119</td>\n","      <td>407.642962</td>\n","      <td>20.200592</td>\n","      <td>79.104019</td>\n","      <td>4.181264</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1.0</td>\n","      <td>5.188777</td>\n","      <td>178.993269</td>\n","      <td>22648.237553</td>\n","      <td>6.318063</td>\n","      <td>326.467898</td>\n","      <td>404.561129</td>\n","      <td>18.840383</td>\n","      <td>89.914438</td>\n","      <td>3.769504</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1.0</td>\n","      <td>7.080795</td>\n","      <td>214.593685</td>\n","      <td>22038.511621</td>\n","      <td>6.199628</td>\n","      <td>294.241392</td>\n","      <td>332.472542</td>\n","      <td>18.469528</td>\n","      <td>46.046287</td>\n","      <td>4.238535</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1.0</td>\n","      <td>9.888941</td>\n","      <td>220.638823</td>\n","      <td>19892.388914</td>\n","      <td>7.986456</td>\n","      <td>326.076560</td>\n","      <td>460.812425</td>\n","      <td>14.929172</td>\n","      <td>59.602808</td>\n","      <td>3.750164</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1.0</td>\n","      <td>10.905076</td>\n","      <td>207.004837</td>\n","      <td>16099.151896</td>\n","      <td>8.186479</td>\n","      <td>369.683867</td>\n","      <td>575.309037</td>\n","      <td>17.556880</td>\n","      <td>80.749849</td>\n","      <td>3.720264</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>1.0</td>\n","      <td>6.332387</td>\n","      <td>186.838028</td>\n","      <td>23073.063966</td>\n","      <td>8.082004</td>\n","      <td>326.980476</td>\n","      <td>233.907965</td>\n","      <td>9.641442</td>\n","      <td>60.940028</td>\n","      <td>5.159002</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>1.0</td>\n","      <td>6.472371</td>\n","      <td>164.768901</td>\n","      <td>24246.503565</td>\n","      <td>7.623012</td>\n","      <td>373.212808</td>\n","      <td>368.079510</td>\n","      <td>12.764141</td>\n","      <td>96.795403</td>\n","      <td>4.134575</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>1.0</td>\n","      <td>7.080795</td>\n","      <td>163.224466</td>\n","      <td>18570.285339</td>\n","      <td>8.109346</td>\n","      <td>296.190262</td>\n","      <td>463.741716</td>\n","      <td>16.900148</td>\n","      <td>61.349832</td>\n","      <td>4.135745</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>1.0</td>\n","      <td>7.071314</td>\n","      <td>213.619625</td>\n","      <td>20038.796417</td>\n","      <td>7.602252</td>\n","      <td>290.057219</td>\n","      <td>605.305041</td>\n","      <td>8.713964</td>\n","      <td>58.413460</td>\n","      <td>4.293727</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>1.0</td>\n","      <td>7.776044</td>\n","      <td>215.533795</td>\n","      <td>38895.356136</td>\n","      <td>7.764668</td>\n","      <td>342.262498</td>\n","      <td>349.431099</td>\n","      <td>6.124625</td>\n","      <td>65.832990</td>\n","      <td>5.425946</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>1.0</td>\n","      <td>8.549693</td>\n","      <td>236.861505</td>\n","      <td>17828.294524</td>\n","      <td>4.752268</td>\n","      <td>341.036337</td>\n","      <td>423.110080</td>\n","      <td>15.313243</td>\n","      <td>80.931305</td>\n","      <td>3.039019</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>1.0</td>\n","      <td>9.485833</td>\n","      <td>218.738236</td>\n","      <td>15357.833373</td>\n","      <td>8.534458</td>\n","      <td>315.831396</td>\n","      <td>429.622212</td>\n","      <td>15.621139</td>\n","      <td>64.826028</td>\n","      <td>3.221130</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>0.0</td>\n","      <td>5.230003</td>\n","      <td>176.714023</td>\n","      <td>27971.891806</td>\n","      <td>7.597981</td>\n","      <td>413.914001</td>\n","      <td>440.355374</td>\n","      <td>14.423614</td>\n","      <td>72.837370</td>\n","      <td>3.045612</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>1.0</td>\n","      <td>8.197353</td>\n","      <td>203.105091</td>\n","      <td>27701.794055</td>\n","      <td>6.472914</td>\n","      <td>328.886838</td>\n","      <td>444.612724</td>\n","      <td>14.250875</td>\n","      <td>62.906205</td>\n","      <td>3.361833</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>1.0</td>\n","      <td>8.128270</td>\n","      <td>231.167537</td>\n","      <td>19954.575554</td>\n","      <td>5.138838</td>\n","      <td>349.067363</td>\n","      <td>386.071149</td>\n","      <td>15.018085</td>\n","      <td>63.340968</td>\n","      <td>4.678742</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>1.0</td>\n","      <td>7.205218</td>\n","      <td>188.911312</td>\n","      <td>31315.614387</td>\n","      <td>8.323041</td>\n","      <td>333.775777</td>\n","      <td>398.918860</td>\n","      <td>15.674210</td>\n","      <td>69.551100</td>\n","      <td>3.465331</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>1.0</td>\n","      <td>5.068796</td>\n","      <td>211.689502</td>\n","      <td>22781.364534</td>\n","      <td>5.330123</td>\n","      <td>317.103903</td>\n","      <td>483.442018</td>\n","      <td>14.495791</td>\n","      <td>77.212274</td>\n","      <td>4.362086</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>1.0</td>\n","      <td>8.922380</td>\n","      <td>278.619448</td>\n","      <td>21963.476003</td>\n","      <td>8.105638</td>\n","      <td>334.053693</td>\n","      <td>385.874799</td>\n","      <td>8.803475</td>\n","      <td>66.396293</td>\n","      <td>5.821826</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>1.0</td>\n","      <td>7.976901</td>\n","      <td>234.779380</td>\n","      <td>15360.835068</td>\n","      <td>5.736027</td>\n","      <td>377.178119</td>\n","      <td>613.633548</td>\n","      <td>14.191274</td>\n","      <td>48.915881</td>\n","      <td>4.753608</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>1.0</td>\n","      <td>7.935607</td>\n","      <td>207.016852</td>\n","      <td>19657.843315</td>\n","      <td>8.604505</td>\n","      <td>312.343607</td>\n","      <td>358.849003</td>\n","      <td>21.228127</td>\n","      <td>63.771833</td>\n","      <td>3.619651</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>1.0</td>\n","      <td>6.359431</td>\n","      <td>208.203021</td>\n","      <td>23347.172710</td>\n","      <td>9.000395</td>\n","      <td>333.775777</td>\n","      <td>336.585610</td>\n","      <td>14.173906</td>\n","      <td>66.396293</td>\n","      <td>3.636495</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>1.0</td>\n","      <td>6.277742</td>\n","      <td>149.563282</td>\n","      <td>15751.449150</td>\n","      <td>8.257751</td>\n","      <td>317.501440</td>\n","      <td>605.398375</td>\n","      <td>15.218378</td>\n","      <td>82.703470</td>\n","      <td>5.255012</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>1.0</td>\n","      <td>7.080795</td>\n","      <td>210.627359</td>\n","      <td>27983.179422</td>\n","      <td>5.947213</td>\n","      <td>333.775777</td>\n","      <td>436.844053</td>\n","      <td>16.127527</td>\n","      <td>64.564044</td>\n","      <td>4.290393</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>1.0</td>\n","      <td>8.447404</td>\n","      <td>163.949905</td>\n","      <td>19583.057349</td>\n","      <td>6.433184</td>\n","      <td>334.655886</td>\n","      <td>547.023830</td>\n","      <td>18.238711</td>\n","      <td>45.668318</td>\n","      <td>5.218307</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>1.0</td>\n","      <td>7.080795</td>\n","      <td>235.240587</td>\n","      <td>10443.796534</td>\n","      <td>7.605519</td>\n","      <td>333.775777</td>\n","      <td>319.842930</td>\n","      <td>8.286550</td>\n","      <td>80.540351</td>\n","      <td>3.900509</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>1.0</td>\n","      <td>7.036752</td>\n","      <td>210.369104</td>\n","      <td>13812.410421</td>\n","      <td>8.378282</td>\n","      <td>342.498361</td>\n","      <td>489.545055</td>\n","      <td>14.823904</td>\n","      <td>69.193358</td>\n","      <td>3.285372</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>1.0</td>\n","      <td>7.449660</td>\n","      <td>193.042081</td>\n","      <td>32251.359648</td>\n","      <td>6.086599</td>\n","      <td>337.135046</td>\n","      <td>572.806143</td>\n","      <td>18.060298</td>\n","      <td>57.401337</td>\n","      <td>4.370404</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>1.0</td>\n","      <td>6.593764</td>\n","      <td>233.514132</td>\n","      <td>25589.610592</td>\n","      <td>9.785696</td>\n","      <td>333.775777</td>\n","      <td>489.703444</td>\n","      <td>14.389609</td>\n","      <td>46.463982</td>\n","      <td>2.346578</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>0.0</td>\n","      <td>7.080795</td>\n","      <td>187.873284</td>\n","      <td>29532.615003</td>\n","      <td>7.981037</td>\n","      <td>274.493395</td>\n","      <td>469.132117</td>\n","      <td>16.169212</td>\n","      <td>78.925527</td>\n","      <td>4.586748</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>1.0</td>\n","      <td>6.742622</td>\n","      <td>209.217344</td>\n","      <td>15488.956748</td>\n","      <td>7.452362</td>\n","      <td>351.052242</td>\n","      <td>408.853562</td>\n","      <td>13.921282</td>\n","      <td>70.205241</td>\n","      <td>4.964673</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>1.0</td>\n","      <td>7.806909</td>\n","      <td>216.473169</td>\n","      <td>21299.318657</td>\n","      <td>6.217597</td>\n","      <td>333.775777</td>\n","      <td>524.079706</td>\n","      <td>13.895631</td>\n","      <td>83.146999</td>\n","      <td>5.127911</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>1.0</td>\n","      <td>5.085429</td>\n","      <td>173.565971</td>\n","      <td>33455.240995</td>\n","      <td>6.033247</td>\n","      <td>351.271267</td>\n","      <td>507.476249</td>\n","      <td>17.795957</td>\n","      <td>62.466135</td>\n","      <td>3.961731</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>1.0</td>\n","      <td>5.094164</td>\n","      <td>223.167125</td>\n","      <td>22957.653403</td>\n","      <td>6.977421</td>\n","      <td>350.101275</td>\n","      <td>476.624550</td>\n","      <td>13.034638</td>\n","      <td>78.075923</td>\n","      <td>4.710936</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>1.0</td>\n","      <td>6.823642</td>\n","      <td>170.328172</td>\n","      <td>14053.220679</td>\n","      <td>8.120631</td>\n","      <td>333.775777</td>\n","      <td>544.011075</td>\n","      <td>13.542213</td>\n","      <td>77.227003</td>\n","      <td>3.386363</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>0.0</td>\n","      <td>10.433291</td>\n","      <td>117.791230</td>\n","      <td>22326.892046</td>\n","      <td>8.161505</td>\n","      <td>307.707509</td>\n","      <td>412.986834</td>\n","      <td>12.890709</td>\n","      <td>65.733478</td>\n","      <td>5.057311</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","      \n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d38cc159-92c3-4f74-a5f9-822473d9dbd8')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","      \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","    \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d38cc159-92c3-4f74-a5f9-822473d9dbd8 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d38cc159-92c3-4f74-a5f9-822473d9dbd8');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","  \n","    </div>\n","  </div>\n","  "],"text/plain":["    Actual         ph    Hardness        Solids  Chloramines     Sulfate  \\\n","0      1.0   6.643159  188.913541  32873.820022     6.791509  333.848842   \n","1      1.0   6.615350  179.240661  26392.863612     9.309160  333.775777   \n","2      1.0   5.628407  226.830043  28334.491937     8.012497  293.924273   \n","3      1.0   7.714313  191.692640  19789.636080     7.856557  340.326314   \n","4      1.0   7.830608  213.249258  18821.389612     6.475113  316.572415   \n","5      1.0   6.350290  190.383738  14905.393852     5.537830  333.775777   \n","6      1.0   6.597292  191.787442  25039.354696     7.294577  395.739529   \n","7      1.0   6.903074  206.922504  17947.988114     7.048017  333.775777   \n","8      1.0   7.736313  225.063103  19496.848592     7.158343  289.945985   \n","9      0.0   6.380717  266.015410  21250.935634     4.854335  357.241027   \n","10     1.0   7.890354  180.158098  32160.533923     7.773484  360.283983   \n","11     1.0   9.318614  317.338124  24497.873935     7.597452  357.167217   \n","12     1.0   6.866357  185.631487  13979.381750     7.043425  375.434032   \n","13     1.0   7.017295  170.132446  20169.344252     5.216280  378.226971   \n","14     1.0   5.827541  191.004930  25863.343245     4.823832  316.025513   \n","15     1.0   7.080795  173.474186  18318.720372     7.597654  282.598119   \n","16     1.0   5.188777  178.993269  22648.237553     6.318063  326.467898   \n","17     1.0   7.080795  214.593685  22038.511621     6.199628  294.241392   \n","18     1.0   9.888941  220.638823  19892.388914     7.986456  326.076560   \n","19     1.0  10.905076  207.004837  16099.151896     8.186479  369.683867   \n","20     1.0   6.332387  186.838028  23073.063966     8.082004  326.980476   \n","21     1.0   6.472371  164.768901  24246.503565     7.623012  373.212808   \n","22     1.0   7.080795  163.224466  18570.285339     8.109346  296.190262   \n","23     1.0   7.071314  213.619625  20038.796417     7.602252  290.057219   \n","24     1.0   7.776044  215.533795  38895.356136     7.764668  342.262498   \n","25     1.0   8.549693  236.861505  17828.294524     4.752268  341.036337   \n","26     1.0   9.485833  218.738236  15357.833373     8.534458  315.831396   \n","27     0.0   5.230003  176.714023  27971.891806     7.597981  413.914001   \n","28     1.0   8.197353  203.105091  27701.794055     6.472914  328.886838   \n","29     1.0   8.128270  231.167537  19954.575554     5.138838  349.067363   \n","30     1.0   7.205218  188.911312  31315.614387     8.323041  333.775777   \n","31     1.0   5.068796  211.689502  22781.364534     5.330123  317.103903   \n","32     1.0   8.922380  278.619448  21963.476003     8.105638  334.053693   \n","33     1.0   7.976901  234.779380  15360.835068     5.736027  377.178119   \n","34     1.0   7.935607  207.016852  19657.843315     8.604505  312.343607   \n","35     1.0   6.359431  208.203021  23347.172710     9.000395  333.775777   \n","36     1.0   6.277742  149.563282  15751.449150     8.257751  317.501440   \n","37     1.0   7.080795  210.627359  27983.179422     5.947213  333.775777   \n","38     1.0   8.447404  163.949905  19583.057349     6.433184  334.655886   \n","39     1.0   7.080795  235.240587  10443.796534     7.605519  333.775777   \n","40     1.0   7.036752  210.369104  13812.410421     8.378282  342.498361   \n","41     1.0   7.449660  193.042081  32251.359648     6.086599  337.135046   \n","42     1.0   6.593764  233.514132  25589.610592     9.785696  333.775777   \n","43     0.0   7.080795  187.873284  29532.615003     7.981037  274.493395   \n","44     1.0   6.742622  209.217344  15488.956748     7.452362  351.052242   \n","45     1.0   7.806909  216.473169  21299.318657     6.217597  333.775777   \n","46     1.0   5.085429  173.565971  33455.240995     6.033247  351.271267   \n","47     1.0   5.094164  223.167125  22957.653403     6.977421  350.101275   \n","48     1.0   6.823642  170.328172  14053.220679     8.120631  333.775777   \n","49     0.0  10.433291  117.791230  22326.892046     8.161505  307.707509   \n","\n","    Conductivity  Organic_carbon  Trihalomethanes  Turbidity  \n","0     336.561501       14.706810        67.844849   4.562198  \n","1     496.363562       12.786595        78.262369   4.453443  \n","2     351.161213       15.019241        35.178662   5.050683  \n","3     377.926368       18.917027        58.692204   4.425134  \n","4     405.278738       16.872395        55.905770   3.551568  \n","5     446.840605       13.983567        67.817096   4.265233  \n","6     501.532653       14.695391        80.050031   4.318305  \n","7     601.985223       11.775110        58.176255   4.473887  \n","8     433.974022       15.153817        74.765101   3.700917  \n","9     358.185473       27.006707        59.937785   4.532020  \n","10    344.550619       11.246460        60.292187   3.209588  \n","11    476.510384       12.032377        68.599830   4.642719  \n","12    462.535115       17.194992        74.557790   4.346154  \n","13    512.768439       15.505992        64.624944   4.899611  \n","14    370.357288       18.361858        48.658109   3.179129  \n","15    407.642962       20.200592        79.104019   4.181264  \n","16    404.561129       18.840383        89.914438   3.769504  \n","17    332.472542       18.469528        46.046287   4.238535  \n","18    460.812425       14.929172        59.602808   3.750164  \n","19    575.309037       17.556880        80.749849   3.720264  \n","20    233.907965        9.641442        60.940028   5.159002  \n","21    368.079510       12.764141        96.795403   4.134575  \n","22    463.741716       16.900148        61.349832   4.135745  \n","23    605.305041        8.713964        58.413460   4.293727  \n","24    349.431099        6.124625        65.832990   5.425946  \n","25    423.110080       15.313243        80.931305   3.039019  \n","26    429.622212       15.621139        64.826028   3.221130  \n","27    440.355374       14.423614        72.837370   3.045612  \n","28    444.612724       14.250875        62.906205   3.361833  \n","29    386.071149       15.018085        63.340968   4.678742  \n","30    398.918860       15.674210        69.551100   3.465331  \n","31    483.442018       14.495791        77.212274   4.362086  \n","32    385.874799        8.803475        66.396293   5.821826  \n","33    613.633548       14.191274        48.915881   4.753608  \n","34    358.849003       21.228127        63.771833   3.619651  \n","35    336.585610       14.173906        66.396293   3.636495  \n","36    605.398375       15.218378        82.703470   5.255012  \n","37    436.844053       16.127527        64.564044   4.290393  \n","38    547.023830       18.238711        45.668318   5.218307  \n","39    319.842930        8.286550        80.540351   3.900509  \n","40    489.545055       14.823904        69.193358   3.285372  \n","41    572.806143       18.060298        57.401337   4.370404  \n","42    489.703444       14.389609        46.463982   2.346578  \n","43    469.132117       16.169212        78.925527   4.586748  \n","44    408.853562       13.921282        70.205241   4.964673  \n","45    524.079706       13.895631        83.146999   5.127911  \n","46    507.476249       17.795957        62.466135   3.961731  \n","47    476.624550       13.034638        78.075923   4.710936  \n","48    544.011075       13.542213        77.227003   3.386363  \n","49    412.986834       12.890709        65.733478   5.057311  "]},"execution_count":171,"metadata":{},"output_type":"execute_result"}],"source":["# Indices with common errors\n","\n","common_mismatch = list(set(set(mismatch_indices_svm).intersection(set(mismatch_indices_nn))).intersection(set(mismatch_indices_rf)))\n","print(common_mismatch)\n","\n","df_dic = {\n","    'Actual': y_test.iloc[common_mismatch].values\n","    #'Predicted': y_pred[common_mismatch]\n","}\n","\n","for i in range(0, 9):\n","  X = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']\n","  df_dic[ X[i] ] = X_test.iloc[common_mismatch, i].tolist()\n","\n","# Display mismatched values\n","mismatch_df = pd.DataFrame(df_dic)\n","\n","print(\"Common Mismatched Predictions:\")\n","mismatch_df.head(50)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CBeSHLXuhxxm","outputId":"cd3c1d48-f692-4990-f305-cd4796c62194"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.53\n","[  1   3   7   8  11  12  15  16  19  21  22  23  25  26  28  29  30  33\n","  35  38  40  41  43  45  46  48  52  54  55  57  58  59  60  63  64  66\n","  67  69  72  75  78  80  81  82  83  86  87  91  93  94  96 100 104 105\n"," 106 112 117 120 122 123 126 128 130 133 134 137 140 142 146 147 148 150\n"," 151 157 158 159 162 165 166 168 170 172 173 175 177 178 182 184 185 186\n"," 189 192 197 198 202 204 207 208 211 215 218 219 223 230 233 234 235 239\n"," 240 242 244 245 246 252 255 256 258 259 260 261 262 266 267 270 271 275\n"," 277 278 279 280 282 285 286 287 291 292 294 295 299 302 303 304 305 307\n"," 310 311 314 318 319 322 324 326 327 329 335 336 340 342 344 346 347 348\n"," 349 355 356 357 358 359 360 362 363 364 366 371 373 376 379 380 384 385\n"," 386 389 392 394 397 399 401 403 405 408 411 413 414 416 418 419 423 424\n"," 425 430 432 433 437 439 441 442 444 447 449 452 458 460 465 469 470 471\n"," 475 477 480 481 483 484 487 488 490 491 492 494 497 499 503 505 509 512\n"," 513 514 516 517 520 521 522 523 524 527 528 531 533 535 537 543 552 558\n"," 562 565 566 567 569 571 574 575 580 581 582 583 586 587 588 591 594 595\n"," 597 598 601 604 609 612 613 614 615 616 620 623 625 626 627 630 632 633\n"," 635 640 645 646 647 650 654 656 657 658 659 665 666 668 670 671 672 673\n"," 674 675 676 679 680 681 682 684 687 688 691 692 696 698 699 700 701 704\n"," 705 707 708 709 711 713 721 723 725 729 731 732 733 734 736 741 742 743\n"," 744 746 747 749 752 753 754 756 758 759 760 761 763 765 766 773 774 775\n"," 776 778 779 781 782 787 788 789 790 792 793 797 798]\n"]}],"source":["#gaussianNB\n","gnb_model = GaussianNB()\n","gnb_model.fit(X_train, y_train)\n","y_pred = gnb_model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n","mismatch_indices = np.where(y_pred != y_test)[0]\n","print(mismatch_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":441},"id":"mcBnV-jBc9xM","outputId":"cdcd39fc-4ade-4cfe-9f91-9fc35a61f3a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mismatched Predictions:\n"]},{"data":{"text/html":["\n","  <div id=\"df-d04c73b6-d131-4326-868d-6c4085fb4c10\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Actual</th>\n","      <th>Predicted</th>\n","      <th>ph</th>\n","      <th>Hardness</th>\n","      <th>Solids</th>\n","      <th>Chloramines</th>\n","      <th>Sulfate</th>\n","      <th>Conductivity</th>\n","      <th>Organic_carbon</th>\n","      <th>Trihalomethanes</th>\n","      <th>Turbidity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>6.643159</td>\n","      <td>188.913541</td>\n","      <td>32873.820022</td>\n","      <td>6.791509</td>\n","      <td>333.848842</td>\n","      <td>336.561501</td>\n","      <td>14.706810</td>\n","      <td>67.844849</td>\n","      <td>4.562198</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>6.615350</td>\n","      <td>179.240661</td>\n","      <td>26392.863612</td>\n","      <td>9.309160</td>\n","      <td>333.775777</td>\n","      <td>496.363562</td>\n","      <td>12.786595</td>\n","      <td>78.262369</td>\n","      <td>4.453443</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>11.235426</td>\n","      <td>178.596496</td>\n","      <td>33773.107061</td>\n","      <td>9.063042</td>\n","      <td>327.650960</td>\n","      <td>425.868039</td>\n","      <td>17.986255</td>\n","      <td>58.986652</td>\n","      <td>5.147055</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>7.714313</td>\n","      <td>191.692640</td>\n","      <td>19789.636080</td>\n","      <td>7.856557</td>\n","      <td>340.326314</td>\n","      <td>377.926368</td>\n","      <td>18.917027</td>\n","      <td>58.692204</td>\n","      <td>4.425134</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>6.350290</td>\n","      <td>190.383738</td>\n","      <td>14905.393852</td>\n","      <td>5.537830</td>\n","      <td>333.775777</td>\n","      <td>446.840605</td>\n","      <td>13.983567</td>\n","      <td>67.817096</td>\n","      <td>4.265233</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>6.984406</td>\n","      <td>181.707343</td>\n","      <td>18194.545310</td>\n","      <td>8.263804</td>\n","      <td>371.146458</td>\n","      <td>201.619737</td>\n","      <td>11.267398</td>\n","      <td>68.192658</td>\n","      <td>3.406210</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>6.702547</td>\n","      <td>207.321086</td>\n","      <td>17246.920347</td>\n","      <td>7.708117</td>\n","      <td>304.510230</td>\n","      <td>329.266002</td>\n","      <td>16.217303</td>\n","      <td>28.878601</td>\n","      <td>3.442983</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>7.164478</td>\n","      <td>213.002441</td>\n","      <td>32751.928963</td>\n","      <td>6.292148</td>\n","      <td>333.775777</td>\n","      <td>490.933121</td>\n","      <td>12.683767</td>\n","      <td>58.252613</td>\n","      <td>4.998203</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>5.433466</td>\n","      <td>177.828302</td>\n","      <td>31421.731633</td>\n","      <td>4.584134</td>\n","      <td>347.097354</td>\n","      <td>490.284674</td>\n","      <td>16.066439</td>\n","      <td>58.416699</td>\n","      <td>2.871196</td>\n","    </tr>\n","    <tr>\n","      <th>199</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>6.865569</td>\n","      <td>231.445054</td>\n","      <td>22585.788809</td>\n","      <td>5.676387</td>\n","      <td>333.775777</td>\n","      <td>496.603425</td>\n","      <td>16.154964</td>\n","      <td>91.461709</td>\n","      <td>4.916218</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200 rows × 11 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","      \n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d04c73b6-d131-4326-868d-6c4085fb4c10')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","      \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","    \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d04c73b6-d131-4326-868d-6c4085fb4c10 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d04c73b6-d131-4326-868d-6c4085fb4c10');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","  \n","    </div>\n","  </div>\n","  "],"text/plain":["     Actual  Predicted         ph    Hardness        Solids  Chloramines  \\\n","0       1.0        0.0   6.643159  188.913541  32873.820022     6.791509   \n","1       1.0        0.0   6.615350  179.240661  26392.863612     9.309160   \n","2       0.0        1.0  11.235426  178.596496  33773.107061     9.063042   \n","3       1.0        0.0   7.714313  191.692640  19789.636080     7.856557   \n","4       1.0        0.0   6.350290  190.383738  14905.393852     5.537830   \n","..      ...        ...        ...         ...           ...          ...   \n","195     1.0        0.0   6.984406  181.707343  18194.545310     8.263804   \n","196     1.0        0.0   6.702547  207.321086  17246.920347     7.708117   \n","197     1.0        0.0   7.164478  213.002441  32751.928963     6.292148   \n","198     1.0        0.0   5.433466  177.828302  31421.731633     4.584134   \n","199     1.0        0.0   6.865569  231.445054  22585.788809     5.676387   \n","\n","        Sulfate  Conductivity  Organic_carbon  Trihalomethanes  Turbidity  \n","0    333.848842    336.561501       14.706810        67.844849   4.562198  \n","1    333.775777    496.363562       12.786595        78.262369   4.453443  \n","2    327.650960    425.868039       17.986255        58.986652   5.147055  \n","3    340.326314    377.926368       18.917027        58.692204   4.425134  \n","4    333.775777    446.840605       13.983567        67.817096   4.265233  \n","..          ...           ...             ...              ...        ...  \n","195  371.146458    201.619737       11.267398        68.192658   3.406210  \n","196  304.510230    329.266002       16.217303        28.878601   3.442983  \n","197  333.775777    490.933121       12.683767        58.252613   4.998203  \n","198  347.097354    490.284674       16.066439        58.416699   2.871196  \n","199  333.775777    496.603425       16.154964        91.461709   4.916218  \n","\n","[200 rows x 11 columns]"]},"execution_count":119,"metadata":{},"output_type":"execute_result"}],"source":["# Find indices where predictions do not match actual labels\n","mismatch_indices = np.where(y_pred != y_test)[0]\n","\n","df_dic = {\n","    'Actual': y_test.iloc[mismatch_indices].values,\n","    'Predicted': y_pred[mismatch_indices]\n","}\n","\n","for i in range(0, 9):\n","  X = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']\n","  df_dic[ X[i] ] = X_test.iloc[mismatch_indices, i].tolist()\n","\n","# Display mismatched values\n","mismatch_df = pd.DataFrame(df_dic)\n","\n","print(\"Mismatched Predictions:\")\n","mismatch_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"9DtYytnsd3qt","outputId":"0cb9754e-16f1-4644-db62-476fc89abedc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Matched Predictions:\n"]},{"data":{"text/html":["\n","  <div id=\"df-07e8116c-e1a6-4f23-953d-e3639aa46237\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Actual</th>\n","      <th>Predicted</th>\n","      <th>ph</th>\n","      <th>Hardness</th>\n","      <th>Solids</th>\n","      <th>Chloramines</th>\n","      <th>Sulfate</th>\n","      <th>Conductivity</th>\n","      <th>Organic_carbon</th>\n","      <th>Trihalomethanes</th>\n","      <th>Turbidity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.080795</td>\n","      <td>183.521107</td>\n","      <td>20461.252710</td>\n","      <td>7.333212</td>\n","      <td>333.119476</td>\n","      <td>356.369022</td>\n","      <td>20.179029</td>\n","      <td>67.019903</td>\n","      <td>4.886634</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.846058</td>\n","      <td>224.058877</td>\n","      <td>23264.109968</td>\n","      <td>5.922367</td>\n","      <td>300.402620</td>\n","      <td>387.971336</td>\n","      <td>13.406737</td>\n","      <td>43.075186</td>\n","      <td>2.487969</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.160467</td>\n","      <td>183.089310</td>\n","      <td>6743.346066</td>\n","      <td>3.803036</td>\n","      <td>277.599099</td>\n","      <td>428.036344</td>\n","      <td>9.799625</td>\n","      <td>90.035374</td>\n","      <td>3.884891</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>13.175402</td>\n","      <td>47.432000</td>\n","      <td>19237.949676</td>\n","      <td>8.907020</td>\n","      <td>375.147315</td>\n","      <td>500.245952</td>\n","      <td>12.083896</td>\n","      <td>66.396293</td>\n","      <td>4.106924</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.499489</td>\n","      <td>230.308775</td>\n","      <td>13902.968646</td>\n","      <td>9.619575</td>\n","      <td>352.084333</td>\n","      <td>442.167006</td>\n","      <td>14.740787</td>\n","      <td>66.396293</td>\n","      <td>5.846827</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.080795</td>\n","      <td>139.331152</td>\n","      <td>2912.211247</td>\n","      <td>10.338234</td>\n","      <td>343.318021</td>\n","      <td>532.885196</td>\n","      <td>11.078341</td>\n","      <td>42.172824</td>\n","      <td>4.093098</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.080795</td>\n","      <td>233.846621</td>\n","      <td>32496.640216</td>\n","      <td>6.637384</td>\n","      <td>347.986448</td>\n","      <td>424.649773</td>\n","      <td>13.028000</td>\n","      <td>55.983973</td>\n","      <td>4.296189</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.394397</td>\n","      <td>187.643411</td>\n","      <td>10603.098021</td>\n","      <td>7.840261</td>\n","      <td>352.835640</td>\n","      <td>376.241146</td>\n","      <td>13.374831</td>\n","      <td>58.950002</td>\n","      <td>2.833901</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.417824</td>\n","      <td>243.304691</td>\n","      <td>320.942611</td>\n","      <td>4.598670</td>\n","      <td>336.097981</td>\n","      <td>361.101769</td>\n","      <td>20.421472</td>\n","      <td>87.052576</td>\n","      <td>3.470812</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>9.159660</td>\n","      <td>195.423316</td>\n","      <td>16679.335164</td>\n","      <td>10.110462</td>\n","      <td>301.746411</td>\n","      <td>404.659103</td>\n","      <td>5.196717</td>\n","      <td>55.466759</td>\n","      <td>5.452362</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.150214</td>\n","      <td>174.926741</td>\n","      <td>41489.639814</td>\n","      <td>5.569869</td>\n","      <td>333.775777</td>\n","      <td>425.250046</td>\n","      <td>12.534009</td>\n","      <td>86.683635</td>\n","      <td>3.502371</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.197796</td>\n","      <td>168.640975</td>\n","      <td>12763.267889</td>\n","      <td>6.897594</td>\n","      <td>333.775777</td>\n","      <td>566.250694</td>\n","      <td>12.437571</td>\n","      <td>65.746717</td>\n","      <td>3.844013</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.198068</td>\n","      <td>200.464445</td>\n","      <td>37200.242065</td>\n","      <td>7.401099</td>\n","      <td>311.794887</td>\n","      <td>573.135532</td>\n","      <td>17.238656</td>\n","      <td>74.948217</td>\n","      <td>3.295034</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.240351</td>\n","      <td>201.997196</td>\n","      <td>14462.674308</td>\n","      <td>6.737176</td>\n","      <td>314.043137</td>\n","      <td>534.800988</td>\n","      <td>14.213794</td>\n","      <td>82.945817</td>\n","      <td>3.782972</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.869782</td>\n","      <td>184.066169</td>\n","      <td>18608.656297</td>\n","      <td>7.411035</td>\n","      <td>333.775777</td>\n","      <td>364.577603</td>\n","      <td>17.580726</td>\n","      <td>0.738000</td>\n","      <td>4.219576</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.127710</td>\n","      <td>203.070309</td>\n","      <td>13227.052382</td>\n","      <td>6.310902</td>\n","      <td>362.275707</td>\n","      <td>409.742224</td>\n","      <td>18.277277</td>\n","      <td>78.929579</td>\n","      <td>4.244173</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.152204</td>\n","      <td>207.299260</td>\n","      <td>27000.106880</td>\n","      <td>6.691806</td>\n","      <td>333.775777</td>\n","      <td>350.004978</td>\n","      <td>12.490509</td>\n","      <td>49.852965</td>\n","      <td>4.689843</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.894323</td>\n","      <td>176.319304</td>\n","      <td>11697.606972</td>\n","      <td>8.198106</td>\n","      <td>376.783989</td>\n","      <td>383.019101</td>\n","      <td>12.677488</td>\n","      <td>57.280265</td>\n","      <td>2.865031</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.313774</td>\n","      <td>196.547696</td>\n","      <td>11310.958313</td>\n","      <td>5.310815</td>\n","      <td>364.405067</td>\n","      <td>607.915848</td>\n","      <td>8.457142</td>\n","      <td>76.694580</td>\n","      <td>4.290072</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.868147</td>\n","      <td>199.035973</td>\n","      <td>20434.105735</td>\n","      <td>6.210994</td>\n","      <td>337.916091</td>\n","      <td>549.414983</td>\n","      <td>16.501578</td>\n","      <td>66.396293</td>\n","      <td>4.315898</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.080795</td>\n","      <td>205.638790</td>\n","      <td>39742.970329</td>\n","      <td>4.660528</td>\n","      <td>323.956492</td>\n","      <td>509.546419</td>\n","      <td>11.674850</td>\n","      <td>55.042679</td>\n","      <td>3.916746</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.080795</td>\n","      <td>165.740229</td>\n","      <td>19919.324589</td>\n","      <td>7.278151</td>\n","      <td>351.965054</td>\n","      <td>420.120713</td>\n","      <td>17.776053</td>\n","      <td>46.002340</td>\n","      <td>3.428096</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.201426</td>\n","      <td>190.799079</td>\n","      <td>8918.892600</td>\n","      <td>4.899875</td>\n","      <td>306.495369</td>\n","      <td>331.378952</td>\n","      <td>18.525434</td>\n","      <td>68.547448</td>\n","      <td>3.964450</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.968131</td>\n","      <td>205.982582</td>\n","      <td>19207.311771</td>\n","      <td>7.285998</td>\n","      <td>283.478555</td>\n","      <td>459.176682</td>\n","      <td>17.916964</td>\n","      <td>50.878820</td>\n","      <td>3.030824</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.080795</td>\n","      <td>168.963026</td>\n","      <td>21030.234531</td>\n","      <td>5.564002</td>\n","      <td>404.343077</td>\n","      <td>440.952071</td>\n","      <td>12.352722</td>\n","      <td>72.033644</td>\n","      <td>4.691018</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.167793</td>\n","      <td>200.913755</td>\n","      <td>27141.362903</td>\n","      <td>7.262417</td>\n","      <td>344.161134</td>\n","      <td>525.598788</td>\n","      <td>16.142284</td>\n","      <td>66.231436</td>\n","      <td>4.289395</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.480728</td>\n","      <td>117.057314</td>\n","      <td>28357.942774</td>\n","      <td>4.702821</td>\n","      <td>414.855784</td>\n","      <td>472.059339</td>\n","      <td>18.340212</td>\n","      <td>76.344581</td>\n","      <td>3.714875</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>7.603775</td>\n","      <td>214.682778</td>\n","      <td>11459.622445</td>\n","      <td>10.999995</td>\n","      <td>398.920869</td>\n","      <td>321.069074</td>\n","      <td>12.010676</td>\n","      <td>58.100707</td>\n","      <td>3.584985</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.521921</td>\n","      <td>198.354626</td>\n","      <td>20139.430347</td>\n","      <td>7.177655</td>\n","      <td>333.775777</td>\n","      <td>603.054956</td>\n","      <td>13.773610</td>\n","      <td>84.816771</td>\n","      <td>3.110932</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>9.026788</td>\n","      <td>253.217074</td>\n","      <td>27965.072146</td>\n","      <td>6.588374</td>\n","      <td>373.553235</td>\n","      <td>588.319651</td>\n","      <td>16.145732</td>\n","      <td>59.124451</td>\n","      <td>2.019042</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.980062</td>\n","      <td>164.127180</td>\n","      <td>17625.466412</td>\n","      <td>6.479558</td>\n","      <td>372.748319</td>\n","      <td>492.703416</td>\n","      <td>11.665753</td>\n","      <td>48.710293</td>\n","      <td>3.969463</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>7.080795</td>\n","      <td>164.909072</td>\n","      <td>32768.227966</td>\n","      <td>6.309685</td>\n","      <td>285.560983</td>\n","      <td>527.198675</td>\n","      <td>15.843303</td>\n","      <td>59.728173</td>\n","      <td>3.499453</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.042794</td>\n","      <td>194.046719</td>\n","      <td>16733.124103</td>\n","      <td>7.701926</td>\n","      <td>350.243966</td>\n","      <td>504.925466</td>\n","      <td>19.703422</td>\n","      <td>82.253454</td>\n","      <td>3.965647</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>9.540252</td>\n","      <td>224.054582</td>\n","      <td>31683.864593</td>\n","      <td>11.994290</td>\n","      <td>315.359085</td>\n","      <td>493.836379</td>\n","      <td>19.093599</td>\n","      <td>81.393722</td>\n","      <td>3.329504</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>4.296247</td>\n","      <td>278.147524</td>\n","      <td>19672.487213</td>\n","      <td>4.955218</td>\n","      <td>414.636726</td>\n","      <td>330.067379</td>\n","      <td>11.293187</td>\n","      <td>70.665540</td>\n","      <td>4.163329</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.181449</td>\n","      <td>209.625601</td>\n","      <td>15196.229987</td>\n","      <td>5.994679</td>\n","      <td>338.336431</td>\n","      <td>342.111286</td>\n","      <td>7.922598</td>\n","      <td>71.537953</td>\n","      <td>5.088860</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.762021</td>\n","      <td>173.636742</td>\n","      <td>15548.416674</td>\n","      <td>5.018671</td>\n","      <td>330.390976</td>\n","      <td>520.640774</td>\n","      <td>11.147704</td>\n","      <td>87.481505</td>\n","      <td>3.256301</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.089431</td>\n","      <td>207.267133</td>\n","      <td>11339.166055</td>\n","      <td>8.888347</td>\n","      <td>317.279123</td>\n","      <td>531.768988</td>\n","      <td>11.674318</td>\n","      <td>48.358250</td>\n","      <td>4.366861</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.467401</td>\n","      <td>197.028926</td>\n","      <td>12883.163531</td>\n","      <td>9.264883</td>\n","      <td>347.421266</td>\n","      <td>355.175712</td>\n","      <td>12.762546</td>\n","      <td>67.159532</td>\n","      <td>3.717823</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.465094</td>\n","      <td>241.268138</td>\n","      <td>43958.678130</td>\n","      <td>7.420155</td>\n","      <td>306.020393</td>\n","      <td>544.600567</td>\n","      <td>20.768986</td>\n","      <td>89.646506</td>\n","      <td>3.790524</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.080795</td>\n","      <td>193.462420</td>\n","      <td>18519.162494</td>\n","      <td>8.021803</td>\n","      <td>333.775777</td>\n","      <td>374.317888</td>\n","      <td>13.749363</td>\n","      <td>43.744705</td>\n","      <td>2.676594</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.078336</td>\n","      <td>197.501912</td>\n","      <td>35957.792183</td>\n","      <td>8.130472</td>\n","      <td>379.527706</td>\n","      <td>360.060477</td>\n","      <td>20.098413</td>\n","      <td>41.865960</td>\n","      <td>4.201581</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.844538</td>\n","      <td>233.764343</td>\n","      <td>28689.595778</td>\n","      <td>7.724432</td>\n","      <td>333.775777</td>\n","      <td>410.387264</td>\n","      <td>15.611859</td>\n","      <td>54.299444</td>\n","      <td>5.013463</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.284097</td>\n","      <td>185.705492</td>\n","      <td>26532.737255</td>\n","      <td>6.132546</td>\n","      <td>328.139582</td>\n","      <td>543.181748</td>\n","      <td>17.398465</td>\n","      <td>47.066392</td>\n","      <td>4.800963</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.080795</td>\n","      <td>133.813115</td>\n","      <td>18272.505973</td>\n","      <td>5.911871</td>\n","      <td>333.775777</td>\n","      <td>415.211735</td>\n","      <td>11.504506</td>\n","      <td>92.975762</td>\n","      <td>3.313199</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.755154</td>\n","      <td>196.782784</td>\n","      <td>19024.688671</td>\n","      <td>6.911868</td>\n","      <td>392.800081</td>\n","      <td>338.430624</td>\n","      <td>10.772862</td>\n","      <td>76.617599</td>\n","      <td>2.728800</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.163999</td>\n","      <td>203.781598</td>\n","      <td>34226.072127</td>\n","      <td>9.412035</td>\n","      <td>320.258917</td>\n","      <td>415.577513</td>\n","      <td>16.221044</td>\n","      <td>90.184208</td>\n","      <td>3.354322</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.933111</td>\n","      <td>162.424918</td>\n","      <td>18846.634913</td>\n","      <td>7.085261</td>\n","      <td>333.775777</td>\n","      <td>593.725764</td>\n","      <td>14.977233</td>\n","      <td>60.690580</td>\n","      <td>3.894989</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.652488</td>\n","      <td>145.010172</td>\n","      <td>19871.788448</td>\n","      <td>4.961066</td>\n","      <td>288.052192</td>\n","      <td>545.974994</td>\n","      <td>10.942024</td>\n","      <td>71.727414</td>\n","      <td>3.742090</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.488133</td>\n","      <td>177.353052</td>\n","      <td>24786.050528</td>\n","      <td>8.613304</td>\n","      <td>369.210671</td>\n","      <td>387.868075</td>\n","      <td>14.149473</td>\n","      <td>48.965996</td>\n","      <td>3.474035</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","      \n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07e8116c-e1a6-4f23-953d-e3639aa46237')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","      \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","    \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-07e8116c-e1a6-4f23-953d-e3639aa46237 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-07e8116c-e1a6-4f23-953d-e3639aa46237');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","  \n","    </div>\n","  </div>\n","  "],"text/plain":["    Actual  Predicted         ph    Hardness        Solids  Chloramines  \\\n","0      0.0        0.0   7.080795  183.521107  20461.252710     7.333212   \n","1      0.0        0.0   7.846058  224.058877  23264.109968     5.922367   \n","2      0.0        0.0   7.160467  183.089310   6743.346066     3.803036   \n","3      1.0        1.0  13.175402   47.432000  19237.949676     8.907020   \n","4      0.0        0.0   5.499489  230.308775  13902.968646     9.619575   \n","5      0.0        0.0   7.080795  139.331152   2912.211247    10.338234   \n","6      0.0        0.0   7.080795  233.846621  32496.640216     6.637384   \n","7      0.0        0.0   8.394397  187.643411  10603.098021     7.840261   \n","8      0.0        0.0   7.417824  243.304691    320.942611     4.598670   \n","9      1.0        1.0   9.159660  195.423316  16679.335164    10.110462   \n","10     0.0        0.0   4.150214  174.926741  41489.639814     5.569869   \n","11     0.0        0.0   7.197796  168.640975  12763.267889     6.897594   \n","12     0.0        0.0   5.198068  200.464445  37200.242065     7.401099   \n","13     0.0        0.0   7.240351  201.997196  14462.674308     6.737176   \n","14     0.0        0.0   7.869782  184.066169  18608.656297     7.411035   \n","15     0.0        0.0   8.127710  203.070309  13227.052382     6.310902   \n","16     0.0        0.0   5.152204  207.299260  27000.106880     6.691806   \n","17     0.0        0.0   7.894323  176.319304  11697.606972     8.198106   \n","18     0.0        0.0   8.313774  196.547696  11310.958313     5.310815   \n","19     0.0        0.0   7.868147  199.035973  20434.105735     6.210994   \n","20     0.0        0.0   7.080795  205.638790  39742.970329     4.660528   \n","21     0.0        0.0   7.080795  165.740229  19919.324589     7.278151   \n","22     0.0        0.0  10.201426  190.799079   8918.892600     4.899875   \n","23     0.0        0.0   6.968131  205.982582  19207.311771     7.285998   \n","24     0.0        0.0   7.080795  168.963026  21030.234531     5.564002   \n","25     0.0        0.0   8.167793  200.913755  27141.362903     7.262417   \n","26     0.0        0.0   7.480728  117.057314  28357.942774     4.702821   \n","27     1.0        1.0   7.603775  214.682778  11459.622445    10.999995   \n","28     0.0        0.0   7.521921  198.354626  20139.430347     7.177655   \n","29     0.0        0.0   9.026788  253.217074  27965.072146     6.588374   \n","30     0.0        0.0   6.980062  164.127180  17625.466412     6.479558   \n","31     1.0        1.0   7.080795  164.909072  32768.227966     6.309685   \n","32     0.0        0.0   7.042794  194.046719  16733.124103     7.701926   \n","33     1.0        1.0   9.540252  224.054582  31683.864593    11.994290   \n","34     1.0        1.0   4.296247  278.147524  19672.487213     4.955218   \n","35     0.0        0.0   7.181449  209.625601  15196.229987     5.994679   \n","36     0.0        0.0   7.762021  173.636742  15548.416674     5.018671   \n","37     0.0        0.0   6.089431  207.267133  11339.166055     8.888347   \n","38     0.0        0.0   5.467401  197.028926  12883.163531     9.264883   \n","39     0.0        0.0   6.465094  241.268138  43958.678130     7.420155   \n","40     0.0        0.0   7.080795  193.462420  18519.162494     8.021803   \n","41     0.0        0.0   6.078336  197.501912  35957.792183     8.130472   \n","42     0.0        0.0   1.844538  233.764343  28689.595778     7.724432   \n","43     0.0        0.0   5.284097  185.705492  26532.737255     6.132546   \n","44     0.0        0.0   7.080795  133.813115  18272.505973     5.911871   \n","45     0.0        0.0   7.755154  196.782784  19024.688671     6.911868   \n","46     0.0        0.0   7.163999  203.781598  34226.072127     9.412035   \n","47     0.0        0.0  10.933111  162.424918  18846.634913     7.085261   \n","48     0.0        0.0   6.652488  145.010172  19871.788448     4.961066   \n","49     0.0        0.0   7.488133  177.353052  24786.050528     8.613304   \n","\n","       Sulfate  Conductivity  Organic_carbon  Trihalomethanes  Turbidity  \n","0   333.119476    356.369022       20.179029        67.019903   4.886634  \n","1   300.402620    387.971336       13.406737        43.075186   2.487969  \n","2   277.599099    428.036344        9.799625        90.035374   3.884891  \n","3   375.147315    500.245952       12.083896        66.396293   4.106924  \n","4   352.084333    442.167006       14.740787        66.396293   5.846827  \n","5   343.318021    532.885196       11.078341        42.172824   4.093098  \n","6   347.986448    424.649773       13.028000        55.983973   4.296189  \n","7   352.835640    376.241146       13.374831        58.950002   2.833901  \n","8   336.097981    361.101769       20.421472        87.052576   3.470812  \n","9   301.746411    404.659103        5.196717        55.466759   5.452362  \n","10  333.775777    425.250046       12.534009        86.683635   3.502371  \n","11  333.775777    566.250694       12.437571        65.746717   3.844013  \n","12  311.794887    573.135532       17.238656        74.948217   3.295034  \n","13  314.043137    534.800988       14.213794        82.945817   3.782972  \n","14  333.775777    364.577603       17.580726         0.738000   4.219576  \n","15  362.275707    409.742224       18.277277        78.929579   4.244173  \n","16  333.775777    350.004978       12.490509        49.852965   4.689843  \n","17  376.783989    383.019101       12.677488        57.280265   2.865031  \n","18  364.405067    607.915848        8.457142        76.694580   4.290072  \n","19  337.916091    549.414983       16.501578        66.396293   4.315898  \n","20  323.956492    509.546419       11.674850        55.042679   3.916746  \n","21  351.965054    420.120713       17.776053        46.002340   3.428096  \n","22  306.495369    331.378952       18.525434        68.547448   3.964450  \n","23  283.478555    459.176682       17.916964        50.878820   3.030824  \n","24  404.343077    440.952071       12.352722        72.033644   4.691018  \n","25  344.161134    525.598788       16.142284        66.231436   4.289395  \n","26  414.855784    472.059339       18.340212        76.344581   3.714875  \n","27  398.920869    321.069074       12.010676        58.100707   3.584985  \n","28  333.775777    603.054956       13.773610        84.816771   3.110932  \n","29  373.553235    588.319651       16.145732        59.124451   2.019042  \n","30  372.748319    492.703416       11.665753        48.710293   3.969463  \n","31  285.560983    527.198675       15.843303        59.728173   3.499453  \n","32  350.243966    504.925466       19.703422        82.253454   3.965647  \n","33  315.359085    493.836379       19.093599        81.393722   3.329504  \n","34  414.636726    330.067379       11.293187        70.665540   4.163329  \n","35  338.336431    342.111286        7.922598        71.537953   5.088860  \n","36  330.390976    520.640774       11.147704        87.481505   3.256301  \n","37  317.279123    531.768988       11.674318        48.358250   4.366861  \n","38  347.421266    355.175712       12.762546        67.159532   3.717823  \n","39  306.020393    544.600567       20.768986        89.646506   3.790524  \n","40  333.775777    374.317888       13.749363        43.744705   2.676594  \n","41  379.527706    360.060477       20.098413        41.865960   4.201581  \n","42  333.775777    410.387264       15.611859        54.299444   5.013463  \n","43  328.139582    543.181748       17.398465        47.066392   4.800963  \n","44  333.775777    415.211735       11.504506        92.975762   3.313199  \n","45  392.800081    338.430624       10.772862        76.617599   2.728800  \n","46  320.258917    415.577513       16.221044        90.184208   3.354322  \n","47  333.775777    593.725764       14.977233        60.690580   3.894989  \n","48  288.052192    545.974994       10.942024        71.727414   3.742090  \n","49  369.210671    387.868075       14.149473        48.965996   3.474035  "]},"execution_count":130,"metadata":{},"output_type":"execute_result"}],"source":["# Find indices where predictions match actual labels\n","match_indices = np.where(y_pred == y_test)[0]\n","\n","df_dic = {\n","    'Actual': y_test.iloc[match_indices].values,\n","    'Predicted': y_pred[match_indices]\n","}\n","\n","for i in range(0, 9):\n","  X = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']\n","  df_dic[ X[i] ] = X_test.iloc[match_indices, i].tolist()\n","\n","# Display mismatched values\n","match_df = pd.DataFrame(df_dic)\n","\n","print(\"Matched Predictions:\")\n","match_df.head(50)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"CsoxueiWQ4VO","outputId":"74265d84-5ac9-4055-f048-1af59a54fb74"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Forest Accuracy: 0.68\n","Classification Report for Random Forest:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.70      0.86      0.77       412\n","         1.0       0.61      0.38      0.47       244\n","\n","    accuracy                           0.68       656\n","   macro avg       0.65      0.62      0.62       656\n","weighted avg       0.67      0.68      0.66       656\n","\n","Confusion Matrix for Random Forest:\n","[[353  59]\n"," [152  92]]\n","\n","Logistic Regression Accuracy: 0.63\n","Classification Report for Logistic Regression:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.63      1.00      0.77       412\n","         1.0       0.00      0.00      0.00       244\n","\n","    accuracy                           0.63       656\n","   macro avg       0.31      0.50      0.39       656\n","weighted avg       0.39      0.63      0.48       656\n","\n","Confusion Matrix for Logistic Regression:\n","[[412   0]\n"," [244   0]]\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-25091e745001>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         }\n\u001b[1;32m     13\u001b[0m         \u001b[0mgrid_search_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid_xgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mgrid_search_xgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_xgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1517\u001b[0m             )\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1520\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             _check_call(\n\u001b[0;32m-> 2051\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Hyperparameter tuning and evaluation\n","for name, model in models.items():\n","    if name == 'XGBoost':\n","        param_grid_xgb = {\n","            'learning_rate': [0.01, 0.1, 0.2],\n","            'max_depth': [3, 5, 7],\n","            'min_child_weight': [1, 3, 5],\n","            'gamma': [0, 0.1, 0.2],\n","            'subsample': [0.8, 1.0],\n","            'colsample_bytree': [0.8, 1.0],\n","            'n_estimators': [100, 200, 300]\n","        }\n","        grid_search_xgb = GridSearchCV(model, param_grid_xgb, cv=5, scoring='accuracy')\n","        grid_search_xgb.fit(X_train_scaled, y_train)\n","        best_model = grid_search_xgb.best_estimator_\n","    else:\n","        model.fit(X_train_scaled, y_train)\n","        best_model = model\n","\n","    y_pred = best_model.predict(X_test_scaled)\n","    accuracy = accuracy_score(y_test, y_pred)\n","\n","    print(f\"{name} Accuracy: {accuracy:.2f}\")\n","    print(f\"Classification Report for {name}:\")\n","    print(classification_report(y_test, y_pred))\n","    print(f\"Confusion Matrix for {name}:\")\n","    print(confusion_matrix(y_test, y_pred))\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8aP6qXaQ733"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4zOlZfBPPri"},"outputs":[],"source":["# Define models\n","models = {\n","    'Random Forest': RandomForestClassifier(random_state=42),\n","    'Logistic Regression': LogisticRegression(random_state=42),\n","    'XGBoost': xgb.XGBClassifier(random_state=42),\n","    'SVM': SVC(random_state=42)\n","}\n","\n","# Hyperparameter tuning and evaluation\n","for name, model in models.items():\n","    if name == 'XGBoost':\n","        param_grid_xgb = {\n","            'learning_rate': [0.01, 0.1, 0.2],\n","            'max_depth': [3, 5, 7],\n","            'min_child_weight': [1, 3, 5],\n","            'gamma': [0, 0.1, 0.2],\n","            'subsample': [0.8, 1.0],\n","            'colsample_bytree': [0.8, 1.0],\n","            'n_estimators': [100, 200, 300]\n","        }\n","        grid_search_xgb = GridSearchCV(model, param_grid_xgb, cv=5, scoring='accuracy')\n","        grid_search_xgb.fit(X_train_scaled, y_train)\n","        best_model = grid_search_xgb.best_estimator_\n","    else:\n","        model.fit(X_train_scaled, y_train)\n","        best_model = model\n","\n","    y_pred = best_model.predict(X_test_scaled)\n","    accuracy = accuracy_score(y_test, y_pred)\n","\n","    print(f\"{name} Accuracy: {accuracy:.2f}\")\n","    print(f\"Classification Report for {name}:\")\n","    print(classification_report(y_test, y_pred))\n","    print(f\"Confusion Matrix for {name}:\")\n","    print(confusion_matrix(y_test, y_pred))\n","    print()\n","\n","# Evaluate and compare results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"e9myY9tUJrdu","outputId":"d6f1194e-810a-4ecd-b73d-6ac6e3fd4a34"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Forest Accuracy: 0.68\n","Classification Report for Random Forest:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.70      0.86      0.77       412\n","         1.0       0.61      0.38      0.47       244\n","\n","    accuracy                           0.68       656\n","   macro avg       0.65      0.62      0.62       656\n","weighted avg       0.67      0.68      0.66       656\n","\n","Confusion Matrix for Random Forest:\n","[[353  59]\n"," [152  92]]\n","\n","Logistic Regression Accuracy: 0.63\n","Classification Report for Logistic Regression:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.63      1.00      0.77       412\n","         1.0       0.00      0.00      0.00       244\n","\n","    accuracy                           0.63       656\n","   macro avg       0.31      0.50      0.39       656\n","weighted avg       0.39      0.63      0.48       656\n","\n","Confusion Matrix for Logistic Regression:\n","[[412   0]\n"," [244   0]]\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-721e6f102daf>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         }\n\u001b[1;32m     47\u001b[0m         \u001b[0mgrid_search_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid_xgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mgrid_search_xgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_xgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1517\u001b[0m             )\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1520\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             _check_call(\n\u001b[0;32m-> 2051\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Define models\n","models = {\n","    'Random Forest': RandomForestClassifier(random_state=42),\n","    'Logistic Regression': LogisticRegression(random_state=42),\n","    'XGBoost': xgb.XGBClassifier(random_state=42),\n","    'SVM': SVC(random_state=42, probability=True)  # Enable probability for VotingClassifier\n","}\n","\n","# Hyperparameter tuning and evaluation\n","for name, model in models.items():\n","    if name == 'XGBoost':\n","        param_grid_xgb = {\n","            'learning_rate': [0.01, 0.1, 0.2],\n","            'max_depth': [3, 5, 7],\n","            'min_child_weight': [1, 3, 5],\n","            'gamma': [0, 0.1, 0.2],\n","            'subsample': [0.8, 1.0],\n","            'colsample_bytree': [0.8, 1.0],\n","            'n_estimators': [100, 200, 300]\n","        }\n","        grid_search_xgb = GridSearchCV(model, param_grid_xgb, cv=5, scoring='accuracy')\n","        grid_search_xgb.fit(X_train_scaled, y_train)\n","        best_model = grid_search_xgb.best_estimator_\n","    else:\n","        model.fit(X_train_scaled, y_train)\n","        best_model = model\n","\n","    y_pred = best_model.predict(X_test_scaled)\n","    accuracy = accuracy_score(y_test, y_pred)\n","\n","    print(f\"{name} Accuracy: {accuracy:.2f}\")\n","    print(f\"Classification Report for {name}:\")\n","    print(classification_report(y_test, y_pred))\n","    print(f\"Confusion Matrix for {name}:\")\n","    print(confusion_matrix(y_test, y_pred))\n","    print()\n","\n","# Ensemble model (VotingClassifier example)\n","ensemble_model = VotingClassifier(estimators=[('RF', models['Random Forest']),\n","                                              ('LR', models['Logistic Regression']),\n","                                              ('XGB', models['XGBoost']),\n","                                              ('SVM', models['SVM'])],\n","                                  voting='soft')  # Soft voting for probabilities\n","ensemble_model.fit(X_train_scaled, y_train)\n","ensemble_pred = ensemble_model.predict(X_test_scaled)\n","ensemble_accuracy = accuracy_score(y_test, ensemble_pred)\n","\n","print(f\"Ensemble Model Accuracy: {ensemble_accuracy:.2f}\")\n","print(f\"Classification Report for Ensemble Model:\")\n","print(classification_report(y_test, ensemble_pred))\n","print(f\"Confusion Matrix for Ensemble Model:\")\n","print(confusion_matrix(y_test, ensemble_pred))\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}